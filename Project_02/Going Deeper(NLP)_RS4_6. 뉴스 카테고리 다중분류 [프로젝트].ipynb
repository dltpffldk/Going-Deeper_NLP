{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "844cea20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 모듈을 임포트하고, 데이터를 다운로드 하기\n",
    "\n",
    "# data 및 전처리 \n",
    "from tensorflow.keras.datasets import reuters\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "# Vectorization 모듈\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "\n",
    "# 모델링 리스트\n",
    "from sklearn.naive_bayes import MultinomialNB # 다항분포 나이브 베이즈 모델\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad12b15",
   "metadata": {},
   "source": [
    "# 조건 1 : 모든 단어로 단어장 구성하여 결과 보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45983b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data, x_test data 확인\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=None, test_split=0.2)\n",
    "\n",
    "print(x_train[0])\n",
    "print(x_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ae270ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/reuters_word_index.json\n",
      "557056/550378 [==============================] - 0s 0us/step\n",
      "565248/550378 [==============================] - 0s 0us/step\n",
      "<sos> <unk> <unk> said as a result of its december acquisition of space co it expects earnings per share in 1987 of 1 15 to 1 30 dlrs per share up from 70 cts in 1986 the company said pretax net should rise to nine to 10 mln dlrs from six mln dlrs in 1986 and rental operation revenues to 19 to 22 mln dlrs from 12 5 mln dlrs it said cash flow per share this year should be 2 50 to three dlrs reuter 3\n"
     ]
    }
   ],
   "source": [
    "# 학습, 테스트 데이터 문자 변경 → 시퀀스로 변환한 것을 왜 다시 문자로 바꾸는지 (용현님 질문)\n",
    "\n",
    "# 인덱스를 단어로 바꿔주는 딕셔너리 정의 \n",
    "word_index = reuters.get_word_index(path=\"reuters_word_index.json\")\n",
    "\n",
    "index_to_word = { index+3 : word for word, index in word_index.items() }\n",
    "\n",
    "# index_to_word에 숫자 0은 <pad>, 숫자 1은 <sos>, 숫자 2는 <unk>를 넣어줍니다.\n",
    "for index, token in enumerate((\"<pad>\", \"<sos>\", \"<unk>\")):\n",
    "  index_to_word[index]=token\n",
    "  \n",
    "# 확인\n",
    "print(' '.join([index_to_word[index] for index in x_train[0]]))\n",
    "\n",
    "#  전체 데이터 텍스트로 변경 ##\n",
    "decoded_train = []\n",
    "for i in range(len(x_train)):\n",
    "  t = ' '.join([index_to_word[index] for index in x_train[i]])  ## \n",
    "  decoded_train.append(t)\n",
    "\n",
    "x_train = decoded_train\n",
    "\n",
    "decoded_test = []\n",
    "for i in range(len(x_test)):\n",
    "  text = ' '.join([index_to_word[index] for index in x_test[i]])\n",
    "  decoded_test.append(text)\n",
    "\n",
    "x_test = decoded_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3fafa2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8982, 9670)\n",
      "(8982, 9670)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(8982, 9670)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 위에서 만들어진 텍스트 데이터를 이용해서 벡터화 진행\n",
    "\n",
    "# DTM 생성\n",
    "dtmvector = CountVectorizer()\n",
    "x_train_dtm = dtmvector.fit_transform(x_train)\n",
    "print(x_train_dtm.shape)\n",
    "\n",
    "# Tfidf 행렬 생성\n",
    "tfidf_matrix = TfidfTransformer()\n",
    "x_train_tfidf = tfidf_matrix.fit_transform(x_train_dtm)\n",
    "print(x_train_tfidf.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e6fcc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트데이터/학습데이터 tf-idf 생성 \n",
    "\n",
    "# 학습데이터(train)\n",
    "x_train_dtm = dtmvector.fit_transform(x_train)\n",
    "x_train_tfidf = tfidf_matrix.fit_transform(x_train_dtm)\n",
    "\n",
    "# 테스트데이터(test)\n",
    "x_test_dtm = dtmvector.transform(x_test)\n",
    "x_test_tfidf = tfidf_matrix.transform(x_test_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9bc556eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델별 결과 저장 리스트 생성 \n",
    "\n",
    "result_acc = [] # 정확도 리스트\n",
    "result_f1score = [] # f1score 리스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4bb9a291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 리스트\n",
    "\n",
    "model_nb = MultinomialNB()\n",
    "model_cnb = ComplementNB()\n",
    "model_lr = LogisticRegression(C=10000, penalty='l2', max_iter=3000)\n",
    "model_lsvc = LinearSVC(C=1000, penalty='l1', max_iter=3000, dual=False)\n",
    "model_tree = DecisionTreeClassifier(max_depth=10, random_state=27)\n",
    "model_forest = RandomForestClassifier(n_estimators = 5, random_state=27)\n",
    "model_grbt = GradientBoostingClassifier(random_state=27, verbose=3)\n",
    "voting_classifier = VotingClassifier(estimators=[\n",
    "         ('lr', LogisticRegression(C=10000, max_iter=3000, penalty='l2')),\n",
    "        ('cb', ComplementNB()),\n",
    "        ('grbt', GradientBoostingClassifier(random_state=0))\n",
    "], voting='soft')\n",
    "\n",
    "\n",
    "model_list = [model_nb, model_cnb, model_lr, model_lsvc, model_forest, model_grbt, voting_classifier]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da36990d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 적용\n",
    "for model in model_list:\n",
    "  model.fit(x_train_tfidf, y_train)\n",
    "  y_pred = model.predict(x_test_tfidf)\n",
    "\n",
    "  acc = accuracy_score(y_test, y_pred)\n",
    "  f_score = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "  result_acc.append(acc)\n",
    "  result_f1score.append(f_score)\n",
    "\n",
    "\n",
    "result_df = pd.DataFrame(zip(result_acc, result_f1score), index=model_list, columns=['accuracy', 'f1_score'])\n",
    "result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9592f1a5",
   "metadata": {},
   "source": [
    "# 조건 2 : 단어장 크기 5000 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3fed02",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=5000, test_split=0.2)\n",
    "\n",
    "# train, test 문자로 바꾸기\n",
    "decoded_train = []\n",
    "for i in range(len(x_train)):\n",
    "  t = ' '.join([index_to_word[index] for index in x_train[i]])\n",
    "  decoded_train.append(t)\n",
    "\n",
    "x_train = decoded_train\n",
    "\n",
    "decoded_test = []\n",
    "for i in range(len(x_test)):\n",
    "  text = ' '.join([index_to_word[index] for index in x_test[i]])\n",
    "  decoded_test.append(text)\n",
    "\n",
    "x_test = decoded_test\n",
    "\n",
    "# tfidf 생성\n",
    "# train\n",
    "x_train_dtm = dtmvector.fit_transform(x_train)\n",
    "x_train_tfidf = tfidf_matrix.fit_transform(x_train_dtm)\n",
    "\n",
    "# test\n",
    "x_test_dtm = dtmvector.transform(x_test)\n",
    "x_test_tfidf = tfidf_matrix.transform(x_test_dtm)\n",
    "\n",
    "\n",
    "# 모델 별 결과를 저장할 리스트 생성\n",
    "result_acc = [] # 정확도 리스트\n",
    "result_f1score = [] # f1score 리스트\n",
    "\n",
    "# 모델 리스트\n",
    "model_nb = MultinomialNB()\n",
    "model_cnb = ComplementNB()\n",
    "model_lr = LogisticRegression(C=10000, penalty='l2', max_iter=3000)\n",
    "model_lsvc = LinearSVC(C=1000, penalty='l1', max_iter=3000, dual=False)\n",
    "model_tree = DecisionTreeClassifier(max_depth=10, random_state=27)\n",
    "model_forest = RandomForestClassifier(n_estimators = 5, random_state=27)\n",
    "model_grbt = GradientBoostingClassifier(random_state=27, verbose=3)\n",
    "voting_classifier = VotingClassifier(estimators=[\n",
    "         ('lr', LogisticRegression(C=10000, max_iter=3000, penalty='l2')),\n",
    "        ('cb', ComplementNB()),\n",
    "        ('grbt', GradientBoostingClassifier(random_state=0))\n",
    "], voting='soft')\n",
    "\n",
    "\n",
    "model_list = [model_nb, model_cnb, model_lr, model_lsvc, model_forest, model_grbt, voting_classifier]\n",
    "\n",
    "for model in model_list:\n",
    "  model.fit(x_train_tfidf, y_train)\n",
    "  y_pred = model.predict(x_test_tfidf)\n",
    "\n",
    "  acc = accuracy_score(y_test, y_pred)\n",
    "  f_score = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "  result_acc.append(acc)\n",
    "  result_f1score.append(f_score)\n",
    "\n",
    "\n",
    "result_df = pd.DataFrame(zip(result_acc, result_f1score), index=model_list, columns=['accuracy', 'f1_score'])\n",
    "result_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f961808f",
   "metadata": {},
   "source": [
    "# 조건 3 : 단어장 크기 5000, skip_top = 5 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb9cdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=5000, skip_top = 5, test_split=0.2)\n",
    "\n",
    "# train, test 문자로 바꾸기\n",
    "decoded_train = []\n",
    "for i in range(len(x_train)):\n",
    "  t = ' '.join([index_to_word[index] for index in x_train[i]])\n",
    "  decoded_train.append(t)\n",
    "\n",
    "x_train = decoded_train\n",
    "\n",
    "decoded_test = []\n",
    "for i in range(len(x_test)):\n",
    "  text = ' '.join([index_to_word[index] for index in x_test[i]])\n",
    "  decoded_test.append(text)\n",
    "\n",
    "x_test = decoded_test\n",
    "\n",
    "# tfidf 생성\n",
    "# train\n",
    "x_train_dtm = dtmvector.fit_transform(x_train)\n",
    "x_train_tfidf = tfidf_matrix.fit_transform(x_train_dtm)\n",
    "\n",
    "# test\n",
    "x_test_dtm = dtmvector.transform(x_test)\n",
    "x_test_tfidf = tfidf_matrix.transform(x_test_dtm)\n",
    "\n",
    "\n",
    "# 모델 별 결과를 저장할 리스트 생성\n",
    "result_acc = [] # 정확도 리스트\n",
    "result_f1score = [] # f1score 리스트\n",
    "\n",
    "# 모델 리스트\n",
    "model_nb = MultinomialNB()\n",
    "model_cnb = ComplementNB()\n",
    "model_lr = LogisticRegression(C=10000, penalty='l2', max_iter=3000)\n",
    "model_lsvc = LinearSVC(C=1000, penalty='l1', max_iter=3000, dual=False)\n",
    "model_tree = DecisionTreeClassifier(max_depth=10, random_state=27)\n",
    "model_forest = RandomForestClassifier(n_estimators = 5, random_state=27)\n",
    "model_grbt = GradientBoostingClassifier(random_state=27, verbose=3)\n",
    "voting_classifier = VotingClassifier(estimators=[\n",
    "         ('lr', LogisticRegression(C=10000, max_iter=3000, penalty='l2')),\n",
    "        ('cb', ComplementNB()),\n",
    "        ('grbt', GradientBoostingClassifier(random_state=0))\n",
    "], voting='soft')\n",
    "\n",
    "\n",
    "model_list = [model_nb, model_cnb, model_lr, model_lsvc, model_forest, model_grbt, voting_classifier]\n",
    "\n",
    "for model in model_list:\n",
    "  model.fit(x_train_tfidf, y_train)\n",
    "  y_pred = model.predict(x_test_tfidf)\n",
    "\n",
    "  acc = accuracy_score(y_test, y_pred)\n",
    "  f_score = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "  result_acc.append(acc)\n",
    "  result_f1score.append(f_score)\n",
    "\n",
    "\n",
    "result_df = pd.DataFrame(zip(result_acc, result_f1score), index=model_list, columns=['accuracy', 'f1_score'])\n",
    "result_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1add09db",
   "metadata": {},
   "source": [
    "# 조건 4 : 단어장 크기 10000, skip_top = 3 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117a0ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=10000, skip_top = 3, test_split=0.2)\n",
    "\n",
    "# train, test 문자로 바꾸기\n",
    "decoded_train = []\n",
    "for i in range(len(x_train)):\n",
    "  t = ' '.join([index_to_word[index] for index in x_train[i]])\n",
    "  decoded_train.append(t)\n",
    "\n",
    "x_train = decoded_train\n",
    "\n",
    "decoded_test = []\n",
    "for i in range(len(x_test)):\n",
    "  text = ' '.join([index_to_word[index] for index in x_test[i]])\n",
    "  decoded_test.append(text)\n",
    "\n",
    "x_test = decoded_test\n",
    "\n",
    "# tfidf 생성\n",
    "# train\n",
    "x_train_dtm = dtmvector.fit_transform(x_train)\n",
    "x_train_tfidf = tfidf_matrix.fit_transform(x_train_dtm)\n",
    "\n",
    "# test\n",
    "x_test_dtm = dtmvector.transform(x_test)\n",
    "x_test_tfidf = tfidf_matrix.transform(x_test_dtm)\n",
    "\n",
    "\n",
    "# 모델 별 결과를 저장할 리스트 생성\n",
    "result_acc = [] # 정확도 리스트\n",
    "result_f1score = [] # f1score 리스트\n",
    "\n",
    "# 모델 리스트\n",
    "model_nb = MultinomialNB()\n",
    "model_cnb = ComplementNB()\n",
    "model_lr = LogisticRegression(C=10000, penalty='l2', max_iter=3000)\n",
    "model_lsvc = LinearSVC(C=1000, penalty='l1', max_iter=3000, dual=False)\n",
    "model_tree = DecisionTreeClassifier(max_depth=10, random_state=27)\n",
    "model_forest = RandomForestClassifier(n_estimators = 5, random_state=27)\n",
    "model_grbt = GradientBoostingClassifier(random_state=27, verbose=3)\n",
    "voting_classifier = VotingClassifier(estimators=[\n",
    "         ('lr', LogisticRegression(C=10000, max_iter=3000, penalty='l2')),\n",
    "        ('cb', ComplementNB()),\n",
    "        ('grbt', GradientBoostingClassifier(random_state=0))\n",
    "], voting='soft')\n",
    "\n",
    "\n",
    "model_list = [model_nb, model_cnb, model_lr, model_lsvc, model_forest, model_grbt, voting_classifier]\n",
    "\n",
    "for model in model_list:\n",
    "  model.fit(x_train_tfidf, y_train)\n",
    "  y_pred = model.predict(x_test_tfidf)\n",
    "\n",
    "  acc = accuracy_score(y_test, y_pred)\n",
    "  f_score = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "  result_acc.append(acc)\n",
    "  result_f1score.append(f_score)\n",
    "\n",
    "\n",
    "result_df = pd.DataFrame(zip(result_acc, result_f1score), index=model_list, columns=['accuracy', 'f1_score'])\n",
    "result_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f0845c",
   "metadata": {},
   "source": [
    "# 조건 5: RNN을 이용한 딥러닝 모델과 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7770ad60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x_train = tf.keras.preprocessing.sequence.pad_sequences(x_train,\n",
    "                                                        value=0,\n",
    "                                                        padding='pre',\n",
    "                                                        maxlen=max_len)\n",
    "x_test = tf.keras.preprocessing.sequence.pad_sequences(x_test,\n",
    "                                                       value=0,\n",
    "                                                       padding='pre',\n",
    "                                                       maxlen = max_len)\n",
    "vocab_size = vocab_size\n",
    "word_vector_dim = 200\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model.add(tf.keras.layers.LSTM(128))\n",
    "model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(46, activation='softmax')) # 클래스가 총 46개라 마지막층은 46개의 결과가 나와야합니다. \n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc8e617",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='SparseCategoricalCrossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=15, batch_size=64, validation_data=(x_test, y_test), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb64575",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ecb859",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed935e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee99d84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982e6029",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efa151e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb7a6fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d063d0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
