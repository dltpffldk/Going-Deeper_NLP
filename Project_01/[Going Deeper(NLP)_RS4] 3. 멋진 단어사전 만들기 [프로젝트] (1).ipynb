{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51b79837",
   "metadata": {},
   "source": [
    "SentencePiece(unigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "e70c7a1e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n",
      "1.21.4\n",
      "3.4.3\n",
      "0.5.2\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import konlpy\n",
    "\n",
    "print(tf.__version__)\n",
    "print(np.__version__)\n",
    "print(plt.__version__)\n",
    "print(konlpy.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "1b22e613",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/aiffel/aiffel/text_preprocess\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "560a8128",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in '/aiffel/aiffel/text_preprocess': ['[Going Deeper(NLP)_RS4_1]. 텍스트 데이터 다루기.ipynb', '[Going Deeper(NLP)_RS4] 3. 멋진 단어사전 만들기 [프로젝트].ipynb', 'korean_spm.vocab', 'ratings.txt', 'ratings_test.txt.temp', 'ratings_train.txt', 'ratings_test.txt', 'korean_spm.model', '.ipynb_checkpoints']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "cwd = os.getcwd()  # Get the current working directory (cwd)\n",
    "files = os.listdir(cwd)  # Get all the files in that directory\n",
    "print(\"Files in %r: %s\" % (cwd, files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "521ecfd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"/aiffel/aiffel/text_preprocess/ratings_train.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "e5474c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_table(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "5be09c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = open(\"/aiffel/aiffel/text_preprocess/ratings_test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "5995ffb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_table(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "4153646a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                       아 더빙.. 진짜 짜증나네요 목소리\n",
       "1                         흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나\n",
       "2                                         너무재밓었다그래서보는것을추천한다\n",
       "3                             교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정\n",
       "4         사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...\n",
       "                                ...                        \n",
       "199995            오랜만에 평점 로긴했네ㅋㅋ 킹왕짱 쌈뽕한 영화를 만났습니다 강렬하게 육쾌함\n",
       "199996         의지 박약들이나 하는거다 탈영은 일단 주인공 김대희 닮았고 이등병 찐따 OOOO\n",
       "199997                   그림도 좋고 완성도도 높았지만... 보는 내내 불안하게 만든다\n",
       "199998       절대 봐서는 안 될 영화.. 재미도 없고 기분만 잡치고.. 한 세트장에서 다 해먹네\n",
       "199999                                           마무리는 또 왜이래\n",
       "Name: document, Length: 199992, dtype: object"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data = pd.concat([train_data, test_data], ignore_index=True, axis=0)\n",
    "\n",
    "# 결측치 제거\n",
    "all_data.dropna(axis=0, inplace=True)\n",
    "\n",
    "# 중복치 제거\n",
    "all_data.drop_duplicates('document', keep='first')\n",
    "\n",
    "# all_data의 document 컬럼을 sentence에 할당\n",
    "sentence = all_data['document']\n",
    "\n",
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "4f8b206c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min length :  1\n",
      "Max length :  146\n",
      "Average length :  35\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt     # 시각화 라이브러리 matplotlib 모듈 실행\n",
    "\n",
    "min_len = 999  # 최소길이\n",
    "max_len = 0    # 최대길이\n",
    "sum_len = 0    # 모든 문장의 길이를 더해서 총합 \n",
    "\n",
    "for s in sentence:     \n",
    "  if len(s) < min_len :\n",
    "    min_len = len(s)\n",
    "  if len(s) > max_len :\n",
    "    max_len = len(s)\n",
    "  sum_len += len(s)\n",
    "# 각 문장의 최소길이, 최대길이, 평균길이를 출력\n",
    "# 평균길이는 총 길이를 문장의 수로 나눈 값\n",
    "\n",
    "print('Min length : ', min_len)\n",
    "print('Max length : ', max_len)\n",
    "print('Average length : ', sum_len // len(sentence))\n",
    "\n",
    "sen_length_cnt = [0] * max_len\n",
    "for sen in sentence:\n",
    "  sen_length_cnt[len(sen)-1] += 1\n",
    "# 문장 길이별 빈도수를 저장할 리스트 초기화\n",
    "# 각 문장의 최대 길이만큼 리스트의 길이 설정\n",
    "# for문에서는 각 문장의 길이 인덱스로 하여 'sen_length_cnt'의 해당 인덱스 값 1 증가\n",
    "# 'sen_length_cnt' 리스트는 각 문장 길이별 빈도수 저장\n",
    "\n",
    "\n",
    "plt.bar(range(max_len), sen_length_cnt, width=1.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "e1f2744d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89460\n",
      "89460\n"
     ]
    }
   ],
   "source": [
    "# 주어진 문장 'Sentence'와 그에 해당하는 라벨'target'의 배열에서 문장의 길이가\n",
    "# 특정범위 (최소문장'min_len'과 최대 문장'max_len'사이)에 들어가는 문장과 \n",
    "# 그에 해당하는 라벨만 선택하여 새로운 리스트('filtered_sen',''filtered_target)에 추가\n",
    "\n",
    "max_len = 200         # 최대문장길이 \n",
    "min_len = 30          # 최소문장길이 \n",
    "\n",
    "filtered_sen = []\n",
    "filtered_target = []\n",
    "# 길이 조건에 만족하는 문장과 그에 해당하는 라벨을 저장하는 빈 리스트 생성 \n",
    "\n",
    "\n",
    "target = np.array(all_data['label'])\n",
    "# 'all_data['label']에서 라벨 데이터를 가져와 numpy 배열로 변환\n",
    "\n",
    "for s, t in zip(sentence, target):\n",
    "  if (len(str(s)) < max_len) and (len(str(s)) >= min_len) :\n",
    "    filtered_sen.append(s)\n",
    "    filtered_target.append(t)\n",
    "# for문을 돌리면 주어진 문장과 라벨 배열을 같은 인덱스 별로 순회\n",
    "# 각 문장 's'의 길이가 최소 길이 이상이고 최대 길이 미만이면 \n",
    "# 문장 's'와 't'를 각각 filtered_sen, filtered_target 리스트에 추가\n",
    "\n",
    "print(len(filtered_sen))\n",
    "print(len(filtered_target))\n",
    "# 문장 길이 출력\n",
    "# 원하는 길이의 문장을 필터링 함 = 89460개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "70bfaff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=/aiffel/aiffel/text_preprocess/ratings_test.txt.temp --model_prefix=korean_spm --vocab_size=30000 --model_type=unigram\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: /aiffel/aiffel/text_preprocess/ratings_test.txt.temp\n",
      "  input_format: \n",
      "  model_prefix: korean_spm\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 30000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(329) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(178) LOG(INFO) Loading corpus: /aiffel/aiffel/text_preprocess/ratings_test.txt.temp\n",
      "trainer_interface.cc(385) LOG(INFO) Loaded all 89460 sentences\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(405) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(466) LOG(INFO) all chars count=5277221\n",
      "trainer_interface.cc(477) LOG(INFO) Done: 99.9501% characters are covered.\n",
      "trainer_interface.cc(487) LOG(INFO) Alphabet size=1649\n",
      "trainer_interface.cc(488) LOG(INFO) Final character coverage=0.999501\n",
      "trainer_interface.cc(520) LOG(INFO) Done! preprocessed 89460 sentences.\n",
      "unigram_model_trainer.cc(139) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(143) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(194) LOG(INFO) Initialized 305964 seed sentencepieces\n",
      "trainer_interface.cc(526) LOG(INFO) Tokenizing input sentences with whitespace: 89460\n",
      "trainer_interface.cc(537) LOG(INFO) Done! 352747\n",
      "unigram_model_trainer.cc(489) LOG(INFO) Using 352747 sentences for EM training\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=156912 obj=15.6334 num_tokens=839763 num_tokens/piece=5.35181\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=145045 obj=14.5574 num_tokens=844404 num_tokens/piece=5.82167\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=108726 obj=14.6565 num_tokens=879819 num_tokens/piece=8.09208\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=108542 obj=14.599 num_tokens=880316 num_tokens/piece=8.11037\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=81404 obj=14.8373 num_tokens=924048 num_tokens/piece=11.3514\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=81391 obj=14.7736 num_tokens=924220 num_tokens/piece=11.3553\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=61043 obj=15.0386 num_tokens=966124 num_tokens/piece=15.8269\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=61043 obj=14.9743 num_tokens=966133 num_tokens/piece=15.8271\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=45782 obj=15.2712 num_tokens=1010748 num_tokens/piece=22.0774\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=45782 obj=15.2049 num_tokens=1010746 num_tokens/piece=22.0774\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=34336 obj=15.5316 num_tokens=1056423 num_tokens/piece=30.7672\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=34336 obj=15.4629 num_tokens=1056445 num_tokens/piece=30.7679\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=33000 obj=15.5086 num_tokens=1062874 num_tokens/piece=32.2083\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=33000 obj=15.499 num_tokens=1062880 num_tokens/piece=32.2085\n",
      "trainer_interface.cc(615) LOG(INFO) Saving model: korean_spm.model\n",
      "trainer_interface.cc(626) LOG(INFO) Saving vocabs: korean_spm.vocab\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 root root 822552 Jun 26 08:37 korean_spm.model\r\n",
      "-rw-r--r-- 1 root root 611799 Jun 26 08:37 korean_spm.vocab\r\n"
     ]
    }
   ],
   "source": [
    "# 주어진 텍스트 'filtered_sen'을 파일에 기록한 후, SentencePiece를 이용\n",
    "# 이를 토큰화하고, 이를 바탕으로 토큰화 모델에 학습하는 역할을 함.\n",
    "\n",
    "import sentencepiece as spm   # SentencePiece 라이브러리, spm으로 불러옴\n",
    "\n",
    "temp_file_path = '/aiffel/aiffel/text_preprocess/ratings_test.txt.temp'\n",
    "vocab_size = 30000       # 단어장의 크기 \n",
    "# 토큰화할 텍스트 저장된 파일 경로와 생성할 단어장의 크기 설정\n",
    "\n",
    "# 지정한 경로에 한 줄에 문장 하나 있는 파일을 만들어줍니다.\n",
    "with open(temp_file_path, 'w') as f:\n",
    "  for row in filtered_sen:    # filtered_sen'에 있는 각 문장에 대해 반복\n",
    "    f.write(str(row) + '\\n')  # 각 문자을 문자열로 변경하여 파일을 쓴 후, 줄바꿈 추가 → 파일에는 한 줄에 한문장 기록\n",
    "# open함수를 사용하여 'temp_file_path'에 해당하는 파일 쓰기 모드 열기('w')\n",
    "\n",
    "spm.SentencePieceTrainer.Train(\n",
    "    '--input={} --model_prefix=korean_spm --vocab_size={} --model_type=unigram'.format(temp_file_path, vocab_size)\n",
    ")\n",
    "# SentencePieceTrainer의 'Train' 함수를 사용하여 토큰화 모델을 학습\n",
    "# 입력파라미터: 1) 토큰화할 텍스트 파일 경로, 2) 생성할 모델의 이름과 접두사, 3) 단어장의 크기, 4) 모델 타입 (unigram or BPE) 지정\n",
    "\n",
    "!ls -l korean_spm*\n",
    "# 생성된 SentencePiece 모델 파일 ('korean_spm.model'과 'korean_spm.vocab')을 출력하여 확인\n",
    "# '!' 셀 명령을 실행하는 iPython의 커맨드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "d87fdc20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(89460, 103)\n",
      "[[    0     0     0 ... 11062  1213   430]\n",
      " [    0     0     0 ...  1348 13757 13866]\n",
      " [    0     0     0 ... 22664  1323     3]\n",
      " ...\n",
      " [    0     0     0 ...   766 28351  6786]\n",
      " [    0     0     0 ...  6718   128  1878]\n",
      " [    0     0     0 ...   472  1874    70]]\n",
      "30000\n",
      "30000\n"
     ]
    }
   ],
   "source": [
    "# SentencePiece모델을 사용해서 텍스트 데이터를 토큰화하고, \n",
    "# 문장 길이를 통일한 텐서와 단어-인덱스 매핑, 인덱스-단어 매핑 딕셔너리를 생성 \n",
    "# 정리하면, 자연어처리 작업에서 텍스트 데이터를 수치형 데이터로 변환 \n",
    "# 모델에 입력하기 위한 전처리 단계\n",
    "\n",
    "import sentencepiece as spm\n",
    "import tensorflow\n",
    "\n",
    "# SentencePiece 모델 로드\n",
    "s = spm.SentencePieceProcessor()\n",
    "s.Load('korean_spm.model')  \n",
    "# 'korean_spm.model'을 자신이 생성한 모델 파일명으로 변경\n",
    "\n",
    "\n",
    "def sp_tokenize(s, corpus): # sp_tokenize라는 함수 정의, s = 모델객체, corpus =  토큰화할 코퍼스\n",
    "  tensor = []\n",
    "\n",
    "  for sen in corpus:\n",
    "    tensor.append(s.EncodeAsIds(sen))\n",
    "  # 코퍼스의 각 문장을 SentencePiece 모델을 사용해 토큰화하고, 토큰화된 문장을 텐서리스트에 추가\n",
    "  \n",
    "  with open(\"./korean_spm.vocab\", 'r') as f:\n",
    "    vocab = f.readlines()\n",
    "    # SentencePiece 모델에 사용된 단어장(vocab)파일을 읽어 vocab에 저장\n",
    "  \n",
    "  word_index = {} # 딕셔너리와 인덱스를 단어에 매핑하는 'index_word' 딕셔너리 생성\n",
    "  index_word = {} \n",
    "\n",
    "  for idx, line in enumerate(vocab):\n",
    "    word = line.split(\"\\t\")[0]\n",
    "\n",
    "    word_index.update({word:idx})\n",
    "    index_word.update({idx:word})\n",
    "  \n",
    "  tensor = tensorflow.keras.preprocessing.sequence.pad_sequences(tensor, padding='pre')  # padding = prefix 세팅\n",
    "\n",
    "  return tensor, word_index, index_word\n",
    "  \n",
    "# 전체 데이터 전처리 + 토큰화\n",
    "tensor, word_index, index_word = sp_tokenize(s, filtered_sen)\n",
    "# 정의한 'sp_tokenize' 함수를 사용해 코퍼스를 토큰화하고, 텐서와 매핑 딕셔너리 생성 \n",
    "\n",
    "\n",
    "print(tensor.shape)\n",
    "print(tensor)\n",
    "print(len(word_index))\n",
    "print(len(index_word))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "10d4c663",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, None, 300)         9000000   \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 128)               219648    \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 9,223,809\n",
      "Trainable params: 9,223,809\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 데이터 나누기\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, val_x, y_train, val_y = train_test_split(tensor, filtered_target, test_size=0.2)\n",
    "y_train = np.array(y_train)\n",
    "val_y = np.array(val_y)\n",
    "\n",
    "\n",
    "# LSTM 모델 설계\n",
    "import tensorflow as tf\n",
    "\n",
    "vocab_size = vocab_size     # 단어장의 사이즈 설정\n",
    "word_vector_dim = 300       # 임베딩 사이즈 설정\n",
    "\n",
    "model_lstm = tf.keras.Sequential()\n",
    "model_lstm.add(tf.keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "# 첫번째 계층으로 임베딩 계층 추가 → 단어를 대응하는 임베딩 벡터로 변환\n",
    "\n",
    "model_lstm.add(tf.keras.layers.LSTM(128))\n",
    "# lstm 계층 추가\n",
    "\n",
    "model_lstm.add(tf.keras.layers.Dense(32, activation='relu'))\n",
    "# 활성화 함수 relu를 사용한 Dense Layer 추가\n",
    "\n",
    "model_lstm.add(tf.keras.layers.Dense(1, activation='sigmoid')) \n",
    "# 최종 출력은 긍정/부정을 나타내는 1차원 출력 생성\n",
    "\n",
    "model_lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "3b253d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "560/560 [==============================] - 11s 17ms/step - loss: 0.3862 - accuracy: 0.8245 - val_loss: 0.3232 - val_accuracy: 0.8588\n",
      "Epoch 2/2\n",
      "560/560 [==============================] - 9s 16ms/step - loss: 0.2130 - accuracy: 0.9139 - val_loss: 0.3518 - val_accuracy: 0.8547\n"
     ]
    }
   ],
   "source": [
    "# 모델 훈련\n",
    "\n",
    "epochs = 2 \n",
    "# 시간 절약을 위해 2 에포크만 돌림\n",
    "\n",
    "model_lstm.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# 모델 훈련, 옵티마이저: adam, 손실함수: binary_crossentropy, 평가지표: accuracy\n",
    "\n",
    "\n",
    "history_lstm = model_lstm.fit(x_train,\n",
    "                              y_train,\n",
    "                              epochs=epochs,\n",
    "                              batch_size=128,                # 배치사이즈 128\n",
    "                              validation_data=(val_x, val_y),\n",
    "                              verbose=1)\n",
    "\n",
    "# loss : 0.2758\n",
    "# acc : 0.8623"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "c2f4ca37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 프로젝트 정리 \n",
    "\n",
    "# 1차 베이스\n",
    "\n",
    "# 문장길이: 10 ~ 140 \n",
    "# 단어장 크기: 16000\n",
    "# 패딩위치: prefix\n",
    "# 임베딩차원: 200\n",
    "# 배치사이즈: 128\n",
    "# val_loss: 0.2758 \n",
    "# val_acc: 0.8623 \n",
    "\n",
    "\n",
    "# 2차 문장길이 / 단어장 / 임베딩 크게 \n",
    "\n",
    "# 문장길이: 30 ~ 200 \n",
    "# 단어장 크기: 20000\n",
    "# 패딩위치: prefix\n",
    "# 임베딩차원: 300\n",
    "# 배치사이즈: 128\n",
    "# val_loss: 0.2119 \n",
    "# val_acc: 0.8543\n",
    "\n",
    "\n",
    "# 3차 문장길이 / 단어장 / 임베딩 작게\n",
    "\n",
    "# 문장길이: 5 ~ 50 \n",
    "# 단어장 크기: 10000\n",
    "# 패딩위치: prefix\n",
    "# 임베딩차원: 100\n",
    "# 배치사이즈: 128\n",
    "# val_loss: 0.2985 \n",
    "# val_acc: 0.8533\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
